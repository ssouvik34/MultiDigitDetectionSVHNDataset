{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIWfZ8UfzszM"
   },
   "source": [
    "Mounting personal Google Drive to use the dataset from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "T_-AuYnbzQ-e",
    "outputId": "5cb1e592-a3d3-4501-c746-c41fcefc56e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "seeyFw850Itm"
   },
   "source": [
    "Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyK1Xzwoz52p"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYcrLsVW0fSz"
   },
   "source": [
    "Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yruGpyWD2dk3"
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('/content/drive/My Drive/DL/Project1/data/SVHN_single_grey1.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EemFTP8F24NR"
   },
   "source": [
    "Dividing into Training ans Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "AeAdOe3-2hA1",
    "outputId": "b7edebc3-cd20-460c-8d8c-4ea573a6cd0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 32, 32) (42000,)\n",
      "Test set (18000, 32, 32) (18000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RknkWkN3FR_"
   },
   "source": [
    "Plotting the first 10 images and their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "colab_type": "code",
    "id": "_2orC5Fq29zO",
    "outputId": "43f34a7c-8ab3-471d-8187-d7ec48ef58ce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAA9CAYAAACpzLMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhqM4BwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the above image: %s' % (y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqfhUir_3U5g"
   },
   "source": [
    "Randomly plotting a value from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "fOs9b1Kn3O92",
    "outputId": "70853a59-6987-4503-8d16-c72408b3fbc0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAADDCAYAAAAyYdXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAjIKYQIyCmECMgphAjIKYQI/D/+5+WpIdxDfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  8\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_test[10], cmap=\"gray\")    \n",
    "plt.show()\n",
    "print('Label: ', y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcpi7qup3uRI"
   },
   "source": [
    "Reshaping the 32X32 image to 1024 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCvnxVAw3dT-"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1024)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJvSisjp34CH"
   },
   "source": [
    "Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugi10kG532jM"
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYvSb22t4Hzu"
   },
   "source": [
    "Converting y data (labels) into categorical (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cb3-Rz8-3_xr"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MvxvLEw_3IL"
   },
   "source": [
    "Printing the shape of the dataset (after one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b7_TrFTx_9nI",
    "outputId": "40e75096-25b5-419f-e450-64e8d68fd5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024) (18000, 1024) (42000, 10) (18000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__wIIPrwAakw"
   },
   "source": [
    "Printing the first value of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "M3e-BI2BAV4g",
    "outputId": "fee0ab6e-7a6d-4ee4-e6eb-cd863d553353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12968785, 0.11866706, 0.10530196, ..., 0.19477727, 0.19942354,\n",
       "       0.20799099], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWRbKqwHAjgd"
   },
   "source": [
    "Printing the first value of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ppdNqvVDAnI5",
    "outputId": "6f50bbb3-b611-4a8e-df79-d4174962a10f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3TRFyt14gqw"
   },
   "source": [
    "Running a traditional algorithm (KNN Classifier) to predict the image values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "pJjqeMS-4R_a",
    "outputId": "2791f703-4715-4fb8-a44b-2ebd2f5982d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t-gT5Nc14rht",
    "outputId": "2276a640-4a73-4865-9c6e-de8ede89305d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32416666666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24hneqb6-1dh"
   },
   "source": [
    "Here we used the default value for n_neighbours = 5, however let's run it for a few more values to find the optimal value of k (n_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xk1IWoH64z-Y",
    "outputId": "34c30e95-3e81-43af-b2a9-684f35a2711f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 11, 15, 19, 23]\n"
     ]
    }
   ],
   "source": [
    "#List of values for n_neigbours (k)\n",
    "neighbours = [3,7,11,15,19,23]\n",
    "print(neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "oTkmYINtBlUT",
    "outputId": "bef18d40-e87c-4dd5-92b1-797af6919743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K value = 3 Accuracy = 0.3783888888888889\n",
      "K value = 7 Accuracy = 0.28683333333333333\n",
      "K value = 11 Accuracy = 0.23622222222222222\n",
      "K value = 15 Accuracy = 0.201\n",
      "K value = 19 Accuracy = 0.17444444444444446\n",
      "K value = 23 Accuracy = 0.15261111111111111\n",
      "The optimal number of neighbors is 3\n"
     ]
    }
   ],
   "source": [
    "ac_scores = []\n",
    "\n",
    "for k in neighbours:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores = knn.score(X_test, y_test)\n",
    "    print(\"K value = {} Accuracy = {}\".format(k, scores))\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "MSE = [1 - x for x in ac_scores]\n",
    "\n",
    "optimal_k = neighbours[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwmGHnJLnsO1"
   },
   "source": [
    "So, traditional Algorithms like KNN appears to be very inefficient for working with datasets like this one. \n",
    "Another noteworthy point to be mentioned here, is the algorithm took a long time to run here.\n",
    "Still the optimal value of K turns out to be 3, so let's try to evaluate the metrics for the same model since those might be useful for comparison with advenced techniques like Neural Networks in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZtlCwWw5oEin",
    "outputId": "8208595a-7357-4194-b716-e45b8f3764ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4N3KBl1jom3j"
   },
   "outputs": [],
   "source": [
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WarSerlpFmx"
   },
   "source": [
    "Printing the accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TJeSrn0YpOCe",
    "outputId": "9c0ea825-175e-4e4d-d0e8-0e4bc4b176fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3783888888888889\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SStT74sDpVAo"
   },
   "source": [
    "Printing the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "Awmq5ayFpYCa",
    "outputId": "c8eeaa1b-3899-4a79-b586-bfc25778c49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.44      0.51      1814\n",
      "           1       0.62      0.51      0.56      1828\n",
      "           2       0.74      0.40      0.52      1803\n",
      "           3       0.56      0.27      0.37      1719\n",
      "           4       0.78      0.51      0.61      1812\n",
      "           5       0.58      0.25      0.35      1768\n",
      "           6       0.54      0.29      0.37      1832\n",
      "           7       0.80      0.54      0.65      1808\n",
      "           8       0.45      0.25      0.32      1812\n",
      "           9       0.60      0.32      0.41      1804\n",
      "\n",
      "   micro avg       0.64      0.38      0.47     18000\n",
      "   macro avg       0.63      0.38      0.47     18000\n",
      "weighted avg       0.63      0.38      0.47     18000\n",
      " samples avg       0.38      0.38      0.38     18000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjIMbRKWorFF"
   },
   "source": [
    "A remarkable thing to note here is that the execution of KNN is taking a lot of time (about 20 min for the test set with a sepecific value of k).\n",
    "Now let's try out Neural Networks on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWP_srcuvMwK"
   },
   "source": [
    "Let's start by initializing a simple Neural Network and then change or tune the parameters and hyperparameters for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3v50rtqm-yw"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNq2gY_NrV9r"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IbrF2mSXrfvM",
    "outputId": "d0096d4b-d2ae-409e-9af8-dc7875ec2e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 2.3033 - accuracy: 0.1182\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 2.2908 - accuracy: 0.1313\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.2792 - accuracy: 0.1526\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.2647 - accuracy: 0.1792\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 2.2482 - accuracy: 0.2059\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.2297 - accuracy: 0.2361\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.2092 - accuracy: 0.2666\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 2.1860 - accuracy: 0.2890\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.1601 - accuracy: 0.3136\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.1323 - accuracy: 0.3376\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.1019 - accuracy: 0.3588\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 2.0693 - accuracy: 0.3758\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 2.0346 - accuracy: 0.3958\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.9985 - accuracy: 0.4162\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.9613 - accuracy: 0.4305\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.9224 - accuracy: 0.4498\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.8833 - accuracy: 0.4641\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.8440 - accuracy: 0.4821\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.8055 - accuracy: 0.4964\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.7681 - accuracy: 0.5103\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.7315 - accuracy: 0.5235\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.6966 - accuracy: 0.5348\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.6631 - accuracy: 0.5444\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.6312 - accuracy: 0.5566\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.6012 - accuracy: 0.5647\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.5722 - accuracy: 0.5727\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.5447 - accuracy: 0.5797\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.5191 - accuracy: 0.5864\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.4945 - accuracy: 0.5936\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.4707 - accuracy: 0.6006\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.4492 - accuracy: 0.6046\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.4288 - accuracy: 0.6119\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.4086 - accuracy: 0.6154\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.3898 - accuracy: 0.6200\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.3723 - accuracy: 0.6235\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.3546 - accuracy: 0.6298\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.3389 - accuracy: 0.6323\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.3238 - accuracy: 0.6365\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.3091 - accuracy: 0.6386\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2953 - accuracy: 0.6431\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2827 - accuracy: 0.6455\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2701 - accuracy: 0.6485\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.2581 - accuracy: 0.6526\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.2467 - accuracy: 0.6536\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2360 - accuracy: 0.6565\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2259 - accuracy: 0.6591\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2154 - accuracy: 0.6635\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.2062 - accuracy: 0.6628\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.1974 - accuracy: 0.6653\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1889 - accuracy: 0.6682\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.1800 - accuracy: 0.6692\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 1.1732 - accuracy: 0.6703\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1639 - accuracy: 0.6737\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1579 - accuracy: 0.6750\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.1496 - accuracy: 0.6766\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1431 - accuracy: 0.6779\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1368 - accuracy: 0.6795\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1306 - accuracy: 0.6824\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1236 - accuracy: 0.6840\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1177 - accuracy: 0.6826\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1119 - accuracy: 0.6858\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1060 - accuracy: 0.6872\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.1010 - accuracy: 0.6879\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0950 - accuracy: 0.6897\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0897 - accuracy: 0.6902\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0850 - accuracy: 0.6903\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0806 - accuracy: 0.6936\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0756 - accuracy: 0.6940\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0714 - accuracy: 0.6951\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0663 - accuracy: 0.6963\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0623 - accuracy: 0.6961\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0584 - accuracy: 0.6969\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0538 - accuracy: 0.6984\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0498 - accuracy: 0.6992\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0455 - accuracy: 0.6999\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0421 - accuracy: 0.7008\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0380 - accuracy: 0.7019\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0335 - accuracy: 0.7038\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0301 - accuracy: 0.7036\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0263 - accuracy: 0.7047\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0227 - accuracy: 0.7063\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0197 - accuracy: 0.7068\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0162 - accuracy: 0.7082\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0125 - accuracy: 0.7073\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0099 - accuracy: 0.7089\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0059 - accuracy: 0.7100\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 1.0032 - accuracy: 0.7110\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 1.0008 - accuracy: 0.7105\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9966 - accuracy: 0.7130\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.9943 - accuracy: 0.7126\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.9913 - accuracy: 0.7146\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9878 - accuracy: 0.7148\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9861 - accuracy: 0.7140\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.9823 - accuracy: 0.7163\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9800 - accuracy: 0.7177\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.9775 - accuracy: 0.7174\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9738 - accuracy: 0.7187\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9716 - accuracy: 0.7193\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9694 - accuracy: 0.7211\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.9672 - accuracy: 0.7211\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ghY5UHk4rj9r",
    "outputId": "8a0302ee-3f0c-4027-cccb-85630305c85d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 43us/sample - loss: 0.9892 - accuracy: 0.7128\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "929dvl2XsZa-",
    "outputId": "e6a69976-8e0e-4141-8626-d12591207a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7127778\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmsVYbGzvdHu"
   },
   "source": [
    "We got a fair accuracy of 71% by using RELU Activation among other activations. Now let's add a few more layers and see model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yAGJg1owsdI3",
    "outputId": "e3959061-26db-41f0-d341-a3e9574526b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 2.3039 - accuracy: 0.1132\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.2927 - accuracy: 0.1391\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2850 - accuracy: 0.1534\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.2742 - accuracy: 0.1639\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2581 - accuracy: 0.1716\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.2376 - accuracy: 0.1886\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2122 - accuracy: 0.2060\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.1825 - accuracy: 0.2254\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.1492 - accuracy: 0.2480\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.1132 - accuracy: 0.2716\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.0759 - accuracy: 0.2942\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.0354 - accuracy: 0.3131\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.9903 - accuracy: 0.3314\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.9388 - accuracy: 0.3538\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.8825 - accuracy: 0.3752\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.8237 - accuracy: 0.4031\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.7581 - accuracy: 0.4353\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.6981 - accuracy: 0.4575\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.6451 - accuracy: 0.4811\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.5852 - accuracy: 0.5043\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.5241 - accuracy: 0.5258\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4829 - accuracy: 0.5376\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.4451 - accuracy: 0.5486\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.4011 - accuracy: 0.5635\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3683 - accuracy: 0.5748\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.3334 - accuracy: 0.5875\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3098 - accuracy: 0.5918\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2779 - accuracy: 0.6060\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2637 - accuracy: 0.6067\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.2347 - accuracy: 0.6185\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.2163 - accuracy: 0.6246\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2001 - accuracy: 0.6298\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1829 - accuracy: 0.6365\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1621 - accuracy: 0.6444\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1493 - accuracy: 0.6476\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1334 - accuracy: 0.6527\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1233 - accuracy: 0.6560\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1080 - accuracy: 0.6607\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0927 - accuracy: 0.6665\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0818 - accuracy: 0.6690\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0701 - accuracy: 0.6746\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0580 - accuracy: 0.6770\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0505 - accuracy: 0.6790\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0375 - accuracy: 0.6816\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0346 - accuracy: 0.6847\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0221 - accuracy: 0.6885\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0129 - accuracy: 0.6920\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0086 - accuracy: 0.6931\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9971 - accuracy: 0.6971\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9958 - accuracy: 0.6971\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9865 - accuracy: 0.7003\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9734 - accuracy: 0.7049\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9712 - accuracy: 0.7044\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9682 - accuracy: 0.7051\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9582 - accuracy: 0.7101\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9513 - accuracy: 0.7122\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9450 - accuracy: 0.7147\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9380 - accuracy: 0.7151\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9352 - accuracy: 0.7145\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9294 - accuracy: 0.7170\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9197 - accuracy: 0.7218\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9203 - accuracy: 0.7224\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9110 - accuracy: 0.7237\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9065 - accuracy: 0.7250\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9038 - accuracy: 0.7239\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8965 - accuracy: 0.7301\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8972 - accuracy: 0.7281\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8892 - accuracy: 0.7306\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8806 - accuracy: 0.7329\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8834 - accuracy: 0.7320\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8762 - accuracy: 0.7340\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8697 - accuracy: 0.7373\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8666 - accuracy: 0.7374\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8607 - accuracy: 0.7393\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8615 - accuracy: 0.7375\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8550 - accuracy: 0.7408\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8504 - accuracy: 0.7433\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8511 - accuracy: 0.7415\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8400 - accuracy: 0.7472\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8381 - accuracy: 0.7468\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8363 - accuracy: 0.7475\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8336 - accuracy: 0.7481\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8289 - accuracy: 0.7492\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8239 - accuracy: 0.7491\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8213 - accuracy: 0.7510\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8147 - accuracy: 0.7535\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8163 - accuracy: 0.7523\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8110 - accuracy: 0.7538\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8106 - accuracy: 0.7536\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8037 - accuracy: 0.7567\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8002 - accuracy: 0.7577\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8005 - accuracy: 0.7577\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.7961 - accuracy: 0.7583\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7942 - accuracy: 0.7592\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7897 - accuracy: 0.7596\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7874 - accuracy: 0.7612\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7870 - accuracy: 0.7610\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7796 - accuracy: 0.7623\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7779 - accuracy: 0.7642\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7752 - accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "zzjSGQGJv7_2",
    "outputId": "1aca825b-88bc-4425-da0d-3eac98ecb75f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 46us/sample - loss: 0.8624 - accuracy: 0.7431\n",
      "Test accuracy:  0.7430556\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm4bhPUNyVyI"
   },
   "source": [
    "Three hidden layers seems to be be working good in our case. Let's play around with the number of activation units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z2kBV_SZynCs",
    "outputId": "31abb7b7-8eea-473b-856f-cb57c3033967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 2.3037 - accuracy: 0.1048\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2953 - accuracy: 0.1122\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2889 - accuracy: 0.1296\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2810 - accuracy: 0.1555\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2705 - accuracy: 0.1777\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2567 - accuracy: 0.2034\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.2372 - accuracy: 0.2227\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2109 - accuracy: 0.2421\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.1761 - accuracy: 0.2572\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.1322 - accuracy: 0.2726\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.0805 - accuracy: 0.2919\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 2.0226 - accuracy: 0.3165\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.9573 - accuracy: 0.3431\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.8923 - accuracy: 0.3701\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.8250 - accuracy: 0.3940\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.7619 - accuracy: 0.4129\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.7071 - accuracy: 0.4312\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.6676 - accuracy: 0.4424\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.6213 - accuracy: 0.4622\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.5843 - accuracy: 0.4749\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.5358 - accuracy: 0.4978\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4995 - accuracy: 0.5114\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.4693 - accuracy: 0.5233\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.4287 - accuracy: 0.5404\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4067 - accuracy: 0.5468\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.3728 - accuracy: 0.5631\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3525 - accuracy: 0.5697\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.3231 - accuracy: 0.5793\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3021 - accuracy: 0.5866\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2769 - accuracy: 0.5963\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2506 - accuracy: 0.6094\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.2330 - accuracy: 0.6145\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.2199 - accuracy: 0.6158\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1945 - accuracy: 0.6291\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1858 - accuracy: 0.6305\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1659 - accuracy: 0.6387\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1558 - accuracy: 0.6425\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1386 - accuracy: 0.6472\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1291 - accuracy: 0.6504\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1171 - accuracy: 0.6568\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1059 - accuracy: 0.6604\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0857 - accuracy: 0.6659\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0788 - accuracy: 0.6685\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0704 - accuracy: 0.6712\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0616 - accuracy: 0.6735\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0507 - accuracy: 0.6776\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0430 - accuracy: 0.6792\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0342 - accuracy: 0.6828\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0271 - accuracy: 0.6851\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0179 - accuracy: 0.6880\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0145 - accuracy: 0.6910\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0079 - accuracy: 0.6917\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9966 - accuracy: 0.6962\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9886 - accuracy: 0.6989\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9805 - accuracy: 0.6999\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9743 - accuracy: 0.7033\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9671 - accuracy: 0.7058\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9683 - accuracy: 0.7031\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9556 - accuracy: 0.7086\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9518 - accuracy: 0.7103\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9433 - accuracy: 0.7126\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9387 - accuracy: 0.7160\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9361 - accuracy: 0.7160\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9204 - accuracy: 0.7193\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9243 - accuracy: 0.7188\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9185 - accuracy: 0.7203\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9115 - accuracy: 0.7220\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9076 - accuracy: 0.7239\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9048 - accuracy: 0.7265\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8935 - accuracy: 0.7303\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9030 - accuracy: 0.7260\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8873 - accuracy: 0.7319\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8814 - accuracy: 0.7323\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8808 - accuracy: 0.7348\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8742 - accuracy: 0.7356\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8697 - accuracy: 0.7365\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8676 - accuracy: 0.7368\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8628 - accuracy: 0.7388\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8598 - accuracy: 0.7390\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8523 - accuracy: 0.7414\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8474 - accuracy: 0.7436\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8485 - accuracy: 0.7442\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8384 - accuracy: 0.7481\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8367 - accuracy: 0.7474\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8321 - accuracy: 0.7486\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8316 - accuracy: 0.7472\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8228 - accuracy: 0.7522\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8266 - accuracy: 0.7498\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8162 - accuracy: 0.7540\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8257 - accuracy: 0.7505\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8107 - accuracy: 0.7550\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8070 - accuracy: 0.7572\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8017 - accuracy: 0.7574\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8011 - accuracy: 0.7551\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8039 - accuracy: 0.7567\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7972 - accuracy: 0.7581\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8010 - accuracy: 0.7577\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7858 - accuracy: 0.7628\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7900 - accuracy: 0.7606\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7815 - accuracy: 0.7642\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(30))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "QqyppIJqyx2q",
    "outputId": "703dbb22-9801-4515-f466-7c340f72ba72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 45us/sample - loss: 0.8621 - accuracy: 0.7479\n",
      "Test accuracy:  0.74788886\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2WbZtmP1nD8"
   },
   "source": [
    "The above combination is working well. Let's proceed with this combination and try diferent weight utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zRBy46-q3l4d",
    "outputId": "c4e94a90-e1c1-40bf-f3b9-3f8e0eeb62f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 2.2989 - accuracy: 0.1138\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2738 - accuracy: 0.1418\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.2423 - accuracy: 0.1750\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 2.1956 - accuracy: 0.2059\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.1360 - accuracy: 0.2404\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 2.0662 - accuracy: 0.2747\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.9885 - accuracy: 0.3072\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.9132 - accuracy: 0.3363\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.8478 - accuracy: 0.3629\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.7882 - accuracy: 0.3866\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.7354 - accuracy: 0.4095\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.6748 - accuracy: 0.4332\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.6365 - accuracy: 0.4501\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.5954 - accuracy: 0.4670\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.5463 - accuracy: 0.4897\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.5047 - accuracy: 0.5123\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4839 - accuracy: 0.5188\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4513 - accuracy: 0.5303\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.4180 - accuracy: 0.5450\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.3783 - accuracy: 0.5614\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3646 - accuracy: 0.5680\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3312 - accuracy: 0.5775\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.3072 - accuracy: 0.5886\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.2938 - accuracy: 0.5958\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.2673 - accuracy: 0.6032\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2459 - accuracy: 0.6122\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.2302 - accuracy: 0.6174\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.2104 - accuracy: 0.6253\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.1906 - accuracy: 0.6310\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.1814 - accuracy: 0.6342\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.1653 - accuracy: 0.6399\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1434 - accuracy: 0.6480\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.1355 - accuracy: 0.6508\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1196 - accuracy: 0.6575\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.1045 - accuracy: 0.6609\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.0903 - accuracy: 0.6660\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 1.0814 - accuracy: 0.6704\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0715 - accuracy: 0.6739\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0574 - accuracy: 0.6776\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 1.0437 - accuracy: 0.6813\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0294 - accuracy: 0.6867\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0319 - accuracy: 0.6848\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0133 - accuracy: 0.6922\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0045 - accuracy: 0.6942\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 1.0052 - accuracy: 0.6942\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9899 - accuracy: 0.6990\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9781 - accuracy: 0.7032\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9723 - accuracy: 0.7060\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9614 - accuracy: 0.7081\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9625 - accuracy: 0.7101\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9509 - accuracy: 0.7111\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.9441 - accuracy: 0.7143\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9405 - accuracy: 0.7155\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9304 - accuracy: 0.7193\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9260 - accuracy: 0.7199\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.9204 - accuracy: 0.7204\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9132 - accuracy: 0.7222\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.9118 - accuracy: 0.7239\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8988 - accuracy: 0.7275\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8937 - accuracy: 0.7288\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8918 - accuracy: 0.7300\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8881 - accuracy: 0.7308\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8885 - accuracy: 0.7305\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8758 - accuracy: 0.7350\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8738 - accuracy: 0.7360\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8660 - accuracy: 0.7398\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8657 - accuracy: 0.7362\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8558 - accuracy: 0.7405\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8512 - accuracy: 0.7422\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8499 - accuracy: 0.7435\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8447 - accuracy: 0.7445\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8384 - accuracy: 0.7454\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8390 - accuracy: 0.7462\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8335 - accuracy: 0.7472\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.8334 - accuracy: 0.7473\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8241 - accuracy: 0.7525\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.8212 - accuracy: 0.7512\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.8183 - accuracy: 0.7527\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.8129 - accuracy: 0.7566\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8105 - accuracy: 0.7568\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8055 - accuracy: 0.7572\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8062 - accuracy: 0.7561\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8014 - accuracy: 0.7576\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.7937 - accuracy: 0.7596\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7935 - accuracy: 0.7599\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7932 - accuracy: 0.7620\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7852 - accuracy: 0.7635\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7884 - accuracy: 0.7641\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.7772 - accuracy: 0.7660\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7829 - accuracy: 0.7650\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7794 - accuracy: 0.7654\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7718 - accuracy: 0.7675\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7649 - accuracy: 0.7692\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7622 - accuracy: 0.7683\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7636 - accuracy: 0.7685\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 22us/sample - loss: 0.7625 - accuracy: 0.7691\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7586 - accuracy: 0.7723\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 24us/sample - loss: 0.7578 - accuracy: 0.7726\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7496 - accuracy: 0.7756\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 23us/sample - loss: 0.7526 - accuracy: 0.7724\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(40, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(30, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "PlGqxFw44S5x",
    "outputId": "88cb1fb7-2d34-4e18-a4d8-645220cdaea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 48us/sample - loss: 0.8228 - accuracy: 0.7546\n",
      "Test accuracy:  0.75461113\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2-9_f2Q7DXu"
   },
   "source": [
    "Wight initialization with He-Uniform worked really well. Now let's add Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-oEUsty47Mew",
    "outputId": "175d9dbb-da02-4c23-986c-718b36ff3eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 2.3043 - accuracy: 0.1666\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.0202 - accuracy: 0.2904\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.8063 - accuracy: 0.3973\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.6145 - accuracy: 0.4877\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.4588 - accuracy: 0.5541\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.3435 - accuracy: 0.5976\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2521 - accuracy: 0.6284\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.1756 - accuracy: 0.6512\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1168 - accuracy: 0.6695\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0673 - accuracy: 0.6806\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0260 - accuracy: 0.6921\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9935 - accuracy: 0.7014\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9626 - accuracy: 0.7082\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9350 - accuracy: 0.7165\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9126 - accuracy: 0.7215\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8878 - accuracy: 0.7312\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8680 - accuracy: 0.7348\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8519 - accuracy: 0.7394\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8354 - accuracy: 0.7442\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8209 - accuracy: 0.7501\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8104 - accuracy: 0.7493\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8006 - accuracy: 0.7513\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7882 - accuracy: 0.7567\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7711 - accuracy: 0.7624\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7641 - accuracy: 0.7638\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7521 - accuracy: 0.7671\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7472 - accuracy: 0.7700\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7307 - accuracy: 0.7726\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7247 - accuracy: 0.7757\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7200 - accuracy: 0.7784\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7109 - accuracy: 0.7787\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7009 - accuracy: 0.7826\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6952 - accuracy: 0.7843\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6814 - accuracy: 0.7872\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6830 - accuracy: 0.7884\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6772 - accuracy: 0.7904\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6675 - accuracy: 0.7940\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6638 - accuracy: 0.7958\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6552 - accuracy: 0.7965\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6476 - accuracy: 0.7985\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6467 - accuracy: 0.7975\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6413 - accuracy: 0.8025\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6348 - accuracy: 0.8017\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6312 - accuracy: 0.8035\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6260 - accuracy: 0.8064\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6218 - accuracy: 0.8071\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6220 - accuracy: 0.8072\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6091 - accuracy: 0.8111\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6102 - accuracy: 0.8105\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6054 - accuracy: 0.8116\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5996 - accuracy: 0.8141\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5989 - accuracy: 0.8138\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5943 - accuracy: 0.8151\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5931 - accuracy: 0.8168\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5875 - accuracy: 0.8176\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5802 - accuracy: 0.8192\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5804 - accuracy: 0.8214\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5828 - accuracy: 0.8199\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5745 - accuracy: 0.8221\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5688 - accuracy: 0.8242\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5673 - accuracy: 0.8256\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5682 - accuracy: 0.8236\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.5669 - accuracy: 0.8236\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5611 - accuracy: 0.8268\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5551 - accuracy: 0.8260\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5554 - accuracy: 0.8280\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5518 - accuracy: 0.8306\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5489 - accuracy: 0.8301\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5483 - accuracy: 0.8299\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5477 - accuracy: 0.8295\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5433 - accuracy: 0.8316\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5381 - accuracy: 0.8342\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5385 - accuracy: 0.8327\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5357 - accuracy: 0.8321\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5335 - accuracy: 0.8325\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5269 - accuracy: 0.8351\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5290 - accuracy: 0.8338\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5279 - accuracy: 0.8347\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5244 - accuracy: 0.8351\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5247 - accuracy: 0.8370\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5250 - accuracy: 0.8379\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5216 - accuracy: 0.8355\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5164 - accuracy: 0.8391\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5128 - accuracy: 0.8401\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5116 - accuracy: 0.8400\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5109 - accuracy: 0.8417\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5073 - accuracy: 0.8413\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5090 - accuracy: 0.8391\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5050 - accuracy: 0.8424\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5088 - accuracy: 0.8415\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5043 - accuracy: 0.8429\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5065 - accuracy: 0.8427\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5021 - accuracy: 0.8433\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4968 - accuracy: 0.8453\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4973 - accuracy: 0.8454\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4940 - accuracy: 0.8464\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4935 - accuracy: 0.8470\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4922 - accuracy: 0.8479\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4872 - accuracy: 0.8481\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4883 - accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(40, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(30, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xd6kbDQY7hSH",
    "outputId": "69c3ce42-e422-4c95-fca1-f3dc695ff580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 58us/sample - loss: 0.7555 - accuracy: 0.7818\n",
      "Test accuracy:  0.7817778\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3ZHnsj48coc"
   },
   "source": [
    "Batch Normalization boosted the accuracy a lot. Let's check if tuning Optimizer and learning rate can add further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zVeN2wWa7taA",
    "outputId": "5609ec46-525d-4662-e157-d04e8ba90e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 1.6311 - accuracy: 0.4295\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0565 - accuracy: 0.6605\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8983 - accuracy: 0.7149\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8216 - accuracy: 0.7413\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7563 - accuracy: 0.7611\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7299 - accuracy: 0.7708\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6970 - accuracy: 0.7813\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6722 - accuracy: 0.7890\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6580 - accuracy: 0.7936\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6341 - accuracy: 0.8001\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6177 - accuracy: 0.8065\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5959 - accuracy: 0.8126\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5916 - accuracy: 0.8140\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5770 - accuracy: 0.8195\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5706 - accuracy: 0.8208\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.5644 - accuracy: 0.8225\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5560 - accuracy: 0.8253\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5402 - accuracy: 0.8290\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5376 - accuracy: 0.8295\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5286 - accuracy: 0.8344\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5270 - accuracy: 0.8329\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.5211 - accuracy: 0.8372\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5153 - accuracy: 0.8372\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.5049 - accuracy: 0.8411\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.5063 - accuracy: 0.8387\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4998 - accuracy: 0.8417\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5034 - accuracy: 0.8403\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4956 - accuracy: 0.8431\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4890 - accuracy: 0.8454\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4836 - accuracy: 0.8469\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4735 - accuracy: 0.8498\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4758 - accuracy: 0.8484\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4762 - accuracy: 0.8503\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4669 - accuracy: 0.8520\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4655 - accuracy: 0.8519\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4614 - accuracy: 0.8513\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4602 - accuracy: 0.8534\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4547 - accuracy: 0.8550\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4534 - accuracy: 0.8546\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4566 - accuracy: 0.8540\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4491 - accuracy: 0.8585\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4458 - accuracy: 0.8599\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.4436 - accuracy: 0.8592\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4387 - accuracy: 0.8585\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4373 - accuracy: 0.8586\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4382 - accuracy: 0.8603\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.4348 - accuracy: 0.8612\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4357 - accuracy: 0.8595\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4282 - accuracy: 0.8613\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4281 - accuracy: 0.8637\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4235 - accuracy: 0.8644\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4271 - accuracy: 0.8632\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4212 - accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.4216 - accuracy: 0.8643\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4176 - accuracy: 0.8660\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4161 - accuracy: 0.8659\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4153 - accuracy: 0.8672\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4076 - accuracy: 0.8694\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4098 - accuracy: 0.8693\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4062 - accuracy: 0.8693\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4048 - accuracy: 0.8689\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.4010 - accuracy: 0.8700\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.4060 - accuracy: 0.8684\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4042 - accuracy: 0.8705\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4021 - accuracy: 0.8707\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4023 - accuracy: 0.8697\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3981 - accuracy: 0.8730\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.3925 - accuracy: 0.8739\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4017 - accuracy: 0.8706\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3955 - accuracy: 0.8725\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3922 - accuracy: 0.8744\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3917 - accuracy: 0.8743\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3842 - accuracy: 0.8762\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3899 - accuracy: 0.8734\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3788 - accuracy: 0.8771\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.3812 - accuracy: 0.8771\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3853 - accuracy: 0.8755\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3819 - accuracy: 0.8770\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3808 - accuracy: 0.8758\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3805 - accuracy: 0.8779\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3742 - accuracy: 0.8788\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3810 - accuracy: 0.8770\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3818 - accuracy: 0.8763\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3772 - accuracy: 0.8792\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3701 - accuracy: 0.8799\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3804 - accuracy: 0.8765\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3703 - accuracy: 0.8797\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3684 - accuracy: 0.8819\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3717 - accuracy: 0.8807\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3662 - accuracy: 0.8831\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3605 - accuracy: 0.8840\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3735 - accuracy: 0.8791\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3681 - accuracy: 0.8818\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3719 - accuracy: 0.8804\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3704 - accuracy: 0.8794\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.3627 - accuracy: 0.8829\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3675 - accuracy: 0.8806\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3595 - accuracy: 0.8833\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3582 - accuracy: 0.8840\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.3583 - accuracy: 0.8846\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(40, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(30, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = optimizers.Adam(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_LIeDPo78zrr",
    "outputId": "d17e0408-5eee-465f-aafd-f0808732587d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 54us/sample - loss: 0.8052 - accuracy: 0.7792\n",
      "Test accuracy:  0.77916664\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8fv_rPHMx4I"
   },
   "source": [
    "Let's the prediction vs actual for a random data in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5VzUkI4JEJQ"
   },
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "6pfAsErIKXN-",
    "outputId": "deb6cb0e-4113-40d9-eba7-8e83cb7d0a9d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAADDCAYAAAAyYdXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAATYUlEQVR4nO2dXYxV53WG32V+bAPDzzAwHsNgUoyx\nUNVQCVmumgtK6orGlXCkyoovKl9YIRex1Ki5Qa7UplIvUqmJ1YsqFVGQqZTaTutERpbbxkWRLKTK\nMXETQozB1OZ/mAEC9mB+B1Yvzh5n+L53z6xz9pk959D3kdDMrNk/395zFnu/61trfebuEEL8hrtm\negBCdBpyCiES5BRCJMgphEiQUwiRIKcQIqGSU5jZFjM7ZGZHzGx7uwYlxExirc5TmNksAIcBPAbg\nJIC3ATzl7u+W7dPX1+erVq1q9XyVto1eJ9suum9dY4yep5nxRIneH2arMu6ye9PquY8fP47z58/T\nAc0OjZLzCIAj7v5BcdKXAGwFUOoUq1atwt69e2+z3bx5M9uO3ZTZs/OhMluZfWxsrGUbGyNj1qxZ\nme2uu/jDmNlv3boVOg/bl9nYeJitDPbhYvfixo0bIVt0jOzvx44HANeuXcts7D6mx9y0aRM9HlDt\n9WkFgBMTfj5Z2G7DzLaZ2T4z23fu3LkKpxOiHqZdaLv7Dnff6O4b+/r6pvt0QlSmyuvTKQCDE35e\nWdgmJX38steVe+65J7OxR2/Zaw17/Yo+9tm+d999d2i76GtfGexeXL9+PbOx1wM2Rkb0msvOw+4j\ns7G/19y5c0P7svtQ9voUfd1N95/sVbXKk+JtAGvN7DNmNhfAlwDsrnA8ITqClp8U7j5mZs8C+E8A\nswDsdPdftW1kQswQVV6f4O6vA3i9TWMRoiPQjLYQCZWeFM1y8+ZNjI6O3mZjQpKJp3vvvTezlU3o\nXL16NbNFJ46YMGYCkcHE4OXLl+m2zM7uxSeffBI6Dxt3VOwuXLiQjpEFPKLzHNH5p+i+dRbD6Ukh\nRIKcQogEOYUQCXIKIRJqFdrXr1/HqVO3T3ozUTx//vzM1tvbm9nmzZtHz8NEGROITEiymWEmWJnY\nvXLlSmYry/c6e/ZsZrt48WJmSwMTZedh18xE9dKlSzPbAw88QMfY39+f2RYsWJDZohkEURujTKSz\nv2sky3Yy0a8nhRAJcgohEuQUQiTIKYRIqFVoX7t2DYcOHcpsKYsWLcpsbJZ7YGCAnoeJZSZEo7PA\nTAyymebz589nthMnTmQ2ADh58mRmGxkZyWyXLl3KbOyeRVPwly9fntnKRCfLImD3lolddr9Zuna0\nbLUZoR0Zj4S2EE0gpxAiQU4hRIKcQogEOYUQCZWiT2Z2FMAogJsAxtx942TbX7lyBQcOHEiPkW23\nYkXWKYemeZQVn0ejTyzSxCI7LALE0jdOnz6d2T788EM6RpbmwdI3GCxqxq6P1WewCFdZ0wMWBWTb\nsihVtK9V1ToJ9vmp0lMLaE9I9g/cXQ2dxB2DXp+ESKjqFA7gx2b2MzPbxjaY2CGwrDRTiE6i6uvT\n59z9lJktB/CGmb3n7m9O3MDddwDYAQADAwNadVJ0PFVb3Jwqvo6Y2Y/QaLr8Ztn2ly9fxv79+2+z\nsfx+ZpszZ05mK6uniHaiYzCxy9I3WJrGkSNHMtuxY8dC5wWAlStXZrYlS5ZkNiYuWTCAifnh4eHM\nVhYMYA0N2D1ndRdMfDOiaR5ljaqrdEEvo+XXJzObb2Y9498D+CMABybfS4jOp8qToh/Ajwqvng3g\nX9z9P9oyKiFmkCptMz8A8Nk2jkWIjkAhWSESam9ckIq66EwsE19l3eqYKIvm6LOGBEzEDg0NZTYm\ntNNGDeOsWbMms7Fah8HBwczGrpuJajZuNqNdNpPORDULgvT09GQ2VstRZbmxZmakGc2cW08KIRLk\nFEIkyCmESJBTCJFQq9AeGxvDhQsXbrOxxSGjrdjLZqmjM5qs2J+J02hDgvfeey+zpdc7zrp16zIb\nS9VmKfMskMDEMhOX7FqOHz9Ox8jGw7oJshntxYsX02OmVJ2RZgI8GmgpQ08KIRLkFEIkyCmESJBT\nCJFQq9AGYim80fbqZUKbCWgmyNh2H3/8cWZj9dhMaB8+fDizsU6CZeNhqdrR1PGPPvoos7H7yAq9\nzpw5Q8fIZu3ZvWDHrNIhsBmhHT1PdK0+QE8KITLkFEIkyCmESJBTCJEwpdA2s50A/gTAiLv/dmHr\nBfAygNUAjgJ40t351G1CKoxYijGr72V112Xii6V/sxpvlqLOBCsToiwlnM1es8ZlABd+7LpZ+jYL\nMERb0jPKAhZsdp9dI7OxAANLMWezz80I7Wg9d7trtF8AsCWxbQewx93XAthT/CzEHcGUTlG0rPl1\nYt4KYFfx/S4AT7R5XELMGK3OU/S7+3gQ+wwaTQwoRZO0bUB5mxIhOonKn1JvvKyVvrC5+w533+ju\nG+UUohto9UkxbGYD7j5kZgMA8sJfwq1bt7IUZ7ZYOROcTLCWpQMzUcW6ZTMhyVKw2cwuayrGggFl\n6/JFF2mPzsQzGzseCzgwAQzwQASb0WY2lhnAZuyjAZCy/1CjwQQWfCmj1f+6dwN4uvj+aQCvtngc\nITqOKZ3CzF4E8N8A1pnZSTN7BsA3ATxmZu8D+MPiZyHuCKZ8fXL3p0p+9fk2j0WIjkDKV4iE2lPH\nU8FUZXmnMvHF7ExoRW1syS9mY9dSJgSjojraBIydh92Hshl2BhO8bKaaBSfYuNnMedWluKIRzfSY\nk33u9KQQIkFOIUSCnEKIBDmFEAlyCiESao8+pVGAaDc3tl1ZZIdFWFgaBIsgXb16NbOxlAVmi9Zx\nlFFlrbboPWP3piyCwxoSsHoTZmMRKXZ/omkaZREp9ndl1LLmnRB3KnIKIRLkFEIkyCmESJjxNI9W\nt5kMJt5YygITfkwgMiE5Ojqa2VgaQ5nQrrL+W1RUR9cJLBsLC0REgw4sYMGOF21I0Uwr/mjXwDL0\npBAiQU4hRIKcQogEOYUQCa12CPwGgC8DGF/R/Dl3fz1wrEzURQVi1a5xTAQzGxPfTKSzmVQmGlmD\nAoA3UoiK4GjdRdVOgtFARDQzgB1vOoRyZP+q9RQvIO8QCADPu/uG4t+UDiFEt9Bqh0Ah7liqaIpn\nzWy/me00s3y5nQIz22Zm+8xsXzOPQCFmilad4jsA1gDYAGAIwLfKNlSHQNFttDSj7e6ftsczs+8C\neC26b+oYVWZ2y8RSdM27KiKPOThbVmD+/Pl0jKwLYjTNnI2Riepo10AWIAB4gCEq6KNdDKNvD82k\nfkfuT9sbFxStMsf5IoADrRxHiE4kEpJ9EcAmAH1mdhLAXwPYZGYb0GisfBTAV6ZxjELUSqsdAr83\nDWMRoiOQ8hUiofbU8SrCOqWZNe+qjIXVNbMZaSa0y0Qss0dnr6Oz7szG7lkz6e3TscZcq+coO0+V\ncwN6UgiRIacQIkFOIUSCnEKIhNqFdiqCqgjvZhpksfTo6Dp6TOQxccqO18ysOxPQVbZjM83Rum0g\nntZfRZBX2bdsW3bd6Wdlss+dnhRCJMgphEiQUwiRIKcQImHGhXZdMGEc7XgdhQk8Vr8M8BrmaM14\nNFU72uW7bB08Jm6r2KL14Wxfds0A/zxFx1OGnhRCJMgphEiQUwiRIKcQIiFSeTcI4J8B9KNRabfD\n3f/BzHoBvAxgNRrVd0+6+4XJjuXumfhjApGJp2iDtDI7S/VmtugMOxN+bOF11rEc4AK8Sk01S1tn\n+7LzlgUDysRtChPqTEBHa9CjS3aVnYdlOjSTORF5UowB+Lq7rwfwKICvmtl6ANsB7HH3tQD2FD8L\n0fVEmqENufs7xfejAA4CWAFgK4BdxWa7ADwxXYMUok6amqcws9UAfhfAWwD63X2o+NUZNF6v2D7b\nAGwrvm91nELURlhom9kCAK8A+Jq737Z0jTde4ukL/sRmaHIK0Q2EnMLM5qDhEN939x8W5uHx/k/F\n15HpGaIQ9RKJPhkaLW0Ouvu3J/xqN4CnAXyz+Ppq4FhZBIJFJKJ592XpCSwiEe1YN2/evMy2ZEne\nKpe12Gdrv7HW9QC/nmgzhGjkiqV5sH2jjR4Afs+jNka022GdLVcjI/99AH8G4Jdm9vPC9hwazvAD\nM3sGwDEAT07PEIWol0gztL0AysTA59s7HCFmHs1oC5EgpxAiodZ6CjPLBBgTUNG2+WXii6VBRAv7\nmdBeunRpZlu4cGFmu3TpUmZjXfqAuLiN1iVExW50HTugWuODqNCuskQCEBfgzdTx6EkhRIKcQogE\nOYUQCXIKIRJqb1yQEhVA09FynQlEthbdokWLMtvixYszW1ntBIOJ2+iC7NEaC2aLznIDPNuABSLY\n7D7bLhpUaebvGl0zr931FEL8v0JOIUSCnEKIBDmFEAm1Cm13D7VEjxbClwkyJhyZ6GQLv/f09GQ2\nljre358XGl6+fDmzsXRwgM+wM6HNroWJS7Ydm01n25WJ0L6+vsw2ODgY2o6lvLd7bbx27M/Qk0KI\nBDmFEAlyCiES5BRCJFTpEPgNAF8GcLbY9Dl3f32yY926dSsTo9FW8820uWdCnQltNmPLZqrvu+++\nzHb//fdnNjZLXZY6zsQtSz0/d+5cZmOCdXR0NLNFO+2x4ALAr3HFihWZrbe3N7NFhXYz69sxot0k\nU9tkM9yR6NN4h8B3zKwHwM/M7I3id8+7+98HjiFE1xCp0R4CMFR8P2pm4x0ChbgjaUpTJB0CAeBZ\nM9tvZjvNLA/mN/bZZmb7zGzfTK1iJEQzVOkQ+B0AawBsQONJ8i22nzoEim4jNKPNOgS6+/CE338X\nwGuRY6WzsdEmXkywlqUNM6EWFfRMILIZ22XLlmW2U6dOZTYmngHgwoV81YKhoaHMxoIGbIzsPEx8\ns4ZtZfXiLOjAxDe7F9Emd4xoinmZvep/vlOOsqxD4HjLzIIvAjhQaSRCdAhVOgQ+ZWYb0AjTHgXw\nlWkZoRA1U6VD4KRzEkJ0K5rRFiJhxpuhsTAtm6mOplUD8cXFo52/meBk4pI1TTt//jwdIxPaZ8+e\nzWysZpyJbybyT58+ndnYDDm7PoDPXq9bty60XZXa6WbWN4wK7dQ22fSAnhRCJMgphEiQUwiRIKcQ\nIqFWoc1Sx5kQHR4eDtlYwy2ACy1Wjx1dBoylVq9cuTKzXbx4MbMxQQ3w2fTjx49nNpaOztLg2bnZ\nTDULEDz44IN0jA8//HBmW758eWZjwj8qtBnRoAjA7yMj2gUd0JNCiAw5hRAJcgohEuQUQiTIKYRI\nqDX6NHv27KzInUUPWCrCiRMnMhtLgQD4enTsPCznn0WkWJSLNTNg0Z6y5gEsKjUyMpLZzpw5k9lY\n9InBuhhu3rw5s61du5bu/9BDD4XOzWpdoh0dqzYzYHb2t07/DkrzEKIJ5BRCJMgphEiIlKPeY2Y/\nNbNfmNmvzOxvCvtnzOwtMztiZi+bWexFV4gOJyK0rwHY7O6XigYGe83s3wH8BRrN0F4ys38C8Awa\nHT5KWbRoER5//PEpT8jWUGMiltVYlMEK+5kgiy6UzsT86tWrMxtLLwF4esvRo0czG2s+wEQiGyNb\nq4+lpwwMDGS2sv2jwYloYCO6lmEz7ZGqtlKa8knhDcY/UXOKfw5gM4B/K+y7ADxRaSRCdAghTWFm\ns4qmBSMA3gDwvwAuuvt4nOskSroGTmyG1sz/7ELMFCGncPeb7r4BwEoAjwDI0yfL9/20GRrrVyRE\np9FU9MndLwL4CYDfA7DYzMZfZFcCyIuEhehCIq34lwG44e4XzexeAI8B+Ds0nONPAbwE4GkAr051\nrJ6eHmzatOk2G8u7Z7OmbFaZtYAH4kKtrDNeCps1jc4ql8Fm41njAzZbHG2xz5owsAABswH8nrPr\nZjPQ0SAGuxb2mSirm2D2qkI7En0aALDLzGah8WT5gbu/ZmbvAnjJzP4WwP+g0UVQiK4n0gxtPxqd\nxlP7B2joCyHuKDSjLUSCnEKIBKtzIRUzOwvgGIA+AHl+eHeia+lMprqWB9w97+KAmp3i05M2VjXa\nWPuJpwFdS2dS5Vr0+iREgpxCiISZcoodM3Te6UDX0pm0fC0zoimE6GT0+iREgpxCiITancLMtpjZ\noaKMdXvd56+Cme00sxEzOzDB1mtmb5jZ+8XXJTM5xihmNmhmPzGzd4sy4z8v7F13Pe0uma7VKYqk\nwn8E8McA1qOxwur6OsdQkRcAbEls2wHscfe1APYUP3cDYwC+7u7rATwK4KvF36Ibr2e8ZPqzADYA\n2GJmj6KRzf28uz8I4AIaJdNTUveT4hEAR9z9A3e/jkba+daax9Ay7v4mgF8n5q1olOMCXVSW6+5D\n7v5O8f0ogINoVE923fW0u2S6bqdYAWBiq7/SMtYuot/dh4rvzwDI2/J1OGa2Go1M6LfQpddTpWQ6\nRUK7jXgjvt1VMW4zWwDgFQBfc/ePJ/6um66nSsl0St1OcQrA4ISf74Qy1mEzGwCA4mveELZDKVoW\nvQLg++7+w8LctdcDtKdkum6neBvA2iIqMBfAlwDsrnkM7WY3GuW4QLAstxOwRg3p9wAcdPdvT/hV\n112PmS0zs8XF9+Ml0wfxm5JpoJlrcfda/wH4AoDDaLzz/WXd56849hcBDAG4gcY76jMAlqIRpXkf\nwH8B6J3pcQav5XNovBrtB/Dz4t8XuvF6APwOGiXR+wEcAPBXhf23APwUwBEA/wrg7sjxlOYhRIKE\nthAJcgohEuQUQiTIKYRIkFMIkSCnECJBTiFEwv8BmhyqrvSX5o4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_test_plt = X_test.reshape(X_test.shape[0], 32,32)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_test_plt[968], cmap=\"gray\")    \n",
    "plt.show()\n",
    "print('Label: ', y_test[968])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "CGvM1MRQJ9ot",
    "outputId": "fc635999-3b1d-496c-f8e7-914f164eac4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000000e+00, 5.1972846e-11, 5.8658614e-16, 3.2799264e-14,\n",
       "       2.8898500e-12, 2.3565907e-15, 1.0591713e-09, 4.5593286e-12,\n",
       "       3.0892105e-13, 2.1719824e-08], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[968]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dsvJnrzNJAp"
   },
   "source": [
    "Conclusion: For this dataset, traditional approach like KNN seemed very ineffective in terms of prediction as well as performance. \n",
    "It took 20 mins for KNN to run for a specific k while all Neural Networks ran within 1-2 min. The accuracy in KNN was around 35-40% for the test set while for Neural Networks it went to close to 80% for the test data set. So, we can conclude here that Neural Networks are more suited for datasets like SVHN datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kA-YiOYcNu7q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProjectNeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
